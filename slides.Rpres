<style>
.reveal h1, .reveal h2, .reveal h3 {
  word-wrap: normal;
  -moz-hyphens: none;
}
.small-code pre code {
  font-size: 1em;
}
</style>

Next Word Predictor
========================================================
class: small-code
author: Coursera Data Science Capstone Project, Bertrand Rigaldies, October 3rd, 2016
autosize: true
font-family: 'Helvetica'
transition: rotate
width: 1440
height: 900

```{r setup, include=FALSE}
opts_chunk$set(cache=TRUE, size='small')
```

* An N-grams-based probabilistic [model](#/model) to predict the next most likely words in an english sentence (See example of R output below);
* Using Maximum Likelihood Estimates (MLE) probabilities, and [Stupid Backoff Algorithm](#/predictor)-based prediction;
* Achieving about 55% prediction [success rate](#/performance);
* Deployed on [Shinyapps.io](#/shinyapp).

```{r load-runtime, echo=FALSE}
require(stringi)
require(stringr)
require(data.table)
require(dplyr)
require(xtable)
source('config.R')
source('utils.R')
source('load_model.R')
source('predictor.R')
model = readRDS('model.rds')
partitionRate = 10
sampleRate = 5
testBigramsDTPathName = paste0(g_corpus_directory_en, '/test_bigrams_DT_partition_', sprintf('%.1f', 100-partitionRate), '_sample_', sprintf('%.1f', sampleRate), '.rds')
testBigramsDT = readRDS(file = testBigramsDTPathName)
testTrigramsDTPathName = paste0(g_corpus_directory_en, '/test_trigrams_DT_partition_', sprintf('%.1f', 100-partitionRate), '_sample_', sprintf('%.1f', sampleRate), '.rds')
testTrigramsDT = readRDS(file = testTrigramsDTPathName)
testQuadgramsDTPathName = paste0(g_corpus_directory_en, '/test_quadgrams_DT_partition_', sprintf('%.1f', 100-partitionRate), '_sample_', sprintf('%.1f', sampleRate), '.rds')
testQuadgramsDT = readRDS(file = testQuadgramsDTPathName)
testPentagramsDTPathName = paste0(g_corpus_directory_en, '/test_pentagrams_DT_partition_', sprintf('%.1f', 100-partitionRate), '_sample_', sprintf('%.1f', sampleRate), '.rds')
testPentagramsDT = readRDS(file = testPentagramsDTPathName)
testData = rbind(
        sample_n(testBigramsDT, 25000),
        sample_n(testTrigramsDT, 25000),
        sample_n(testQuadgramsDT, 25000),
        sample_n(testPentagramsDT, 25000)
    )
```

```{r example}
predictWithStupidBackoff(model = model, sentence = 'as far as I can', matches_count_max = 5)
```

The Probabilistic Model
========================================================
id: model
class: small-code

* English Corpus from [HC Corpora](www.corpora.heliohost.org).
* 10% of the English Corpus used as training dataset.
* 5 N-grams tables built from the training dataset, N = 1 to 5.
* Each N-gram table is structured as follows:

Column | Description
------------- | -------------
**gram** | The gram as a string of one ($N=1$) or more ($N\gt2$) words.
**count** | The number of times the gram is used in the Corpus training dataset.
**lowergram** | The preceding lower-order gram when $N\gt2$, or "lowergram" (e.g., "in the" in gram "in the world").
**last_word_in_gram** | The last word in the gram (e.g., "world" in gram "in the world").
**logprob** | Logarithm of the gram's **Maximum Likelihood Estimation** (MLE).
**logpercent** | Logarithm of the gram's **Percent Use** (PU) based on the gram's count.

**Maximum Likelihood Estimation**: $P_{MLE}(w_1 \ldots w_n) = P(w_n|w_1 \ldots w_{n-1}) = \frac{Count(w_1 \ldots w_n)}{Count(w_1 \ldots w_{n-1})}$

**Percent Use**: $P_{PU}(w_1 \ldots w_n) = \frac{Count(w_1 \ldots w_n)}{Count(n-Grams)}$

The Stupid Backoff Algorithm
========================================================
id: predictor

Reference: [Speech and Language Processing, Daniel Jurafsky & James H. martin, January 9, 2015, Chapter 4: N-Grams, section 4.5.1 The Web and Stupid Backoff](https://web.stanford.edu/~jurafsky/slp3/4.pdf)

The basic flow is:

1. Lookup the top 10 matches in the highest order N-gram table in the model, where lowergram = last (N-1) words in the input sentence;
1. If less than 10 matches were found, **back off** and look up for additional matches in the (N-1)-gram table;
1. At each backoff, *discount* the matches' MLE probabilities by applying a $\lambda$ weight. (e.g., $\lambda=0.4$): $P_{MLE(weighted)}(w_{1} \ldots w_{N-1}w_{match}) = \lambda^{(4 - (N-1))} * P_{MLE}(w_{1} \ldots w_{N-1}w_{match})$
1. Repeat the previous step until 10 matches are found, using the unigrams table as the last lookup step.
1. Return the top 10 matches (or predictions), sorted by $\lambda\$-weighted MLE descending.

Quantitative Predictive Performance: ~55%
========================================================
id: performance
class: small-code

* Sample 10% of the non-training data in the English Corpus to create a test dataset;
* Build a bi, tri, quad, and pentagrams from the test dataset;
* For each N-gram, build a `data.table` with the following columns: a) `gram`: The gram; b) `lowergram`: The gram's immediate lower-order gram; c) `word_to_predict`: the gram's last word;
* Sample 25,000 grams from each N-gram (N=2 to 5), and `rbind` the resulting samples into a 100,000-row table (Samples of the resulting test data shown below):
```{r example-test-dataset, echo=FALSE}
set.seed(10)
sample_n(testData, 3)
```
* **Success Criteria**: For each `gram` in the test `data.table`, run the Stupid Backoff prediction on the `lowergram`, and if the resulting 10 predictions contain the gram's `word_to_predict`, then the test is a PASS, else the test is a FAIL (Test samples shown below):
```{r test, echo = FALSE, results="asis"}
set.seed(10);
testSampleSize = 3
testSample = sample_n(testData, testSampleSize)
testPredictions = mapply(function(lowergram) {
        stri_paste(predictWithStupidBackoff(model = model,
                                            sentence = lowergram)$`Predicted Word`, collapse = ', ')
    },
    testSample$lowergram)
testResults = mapply(function(lowergram, word_to_predict) {
        ifelse(word_to_predict %in% predictWithStupidBackoff(model = model,
                                                      sentence = lowergram)$`Predicted Word`,
               'PASS', 'FAIL')
    },
    testSample$lowergram,
    testSample$word_to_predict)
print(xtable(data.table(`Test Gram` = testSample$gram, 
                        `Word to Predict` = testSample$word_to_predict,
                        `Top 10 Predicted Words` = testPredictions,
                        `Test Result` = testResults),
             auto=TRUE
             ),
      comment=F,
      type = "html")
```

My Shinyapps.io Application
========================================================
id: shinyapp

![alt text](shinyappio.jpg)

Connect to my application [here](https://brigaldies.shinyapps.io/NextWordPredictor/).